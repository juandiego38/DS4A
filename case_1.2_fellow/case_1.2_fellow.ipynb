{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BTFHvTBmWx7"
   },
   "source": [
    "# How are trading volume and volatility related for energy stocks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsqrxpICmWx_",
    "tags": [
     "2_min"
    ]
   },
   "source": [
    "## Goals\n",
    "\n",
    "By the end of this case, we will have introduced the `pandas` library within Python. You will also have gained experience with the `numpy` library, know how to read data files, and conduct descriptive statistics.\n",
    "\n",
    "You should also begin to develop a proper mindset for investigating the library on your own, via documentation or other resources such as StackOverflow. Self-research of existing documentation is a crucial part of developing as a data professional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56p_G-1QmWyC",
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "VIuQsUM5mWyE"
   },
   "source": [
    "**Business Context.** You are an analyst at a large bank focused on natural resource stock investments. Natural resources are vital for a variety of industries in our economy. Recently, your division has taken interest in the following stocks:\n",
    "\n",
    "1. Dominion Energy Inc.\n",
    "2. Exelon Corp.\n",
    "3. NextEra Energy Inc.\n",
    "4. Southern Co.\n",
    "5. Duke Energy Corp.\n",
    "\n",
    "These stocks are all part of the energy sector, an important but volatile sector of the stock market. (A stock is considered volatile if its price exhibits large percentage changes day-to-day.) While high volatility increases the chance of great gains, it also makes it more likely to incur large losses, so risk must be managed carefully when investing in high-volatility stocks.\n",
    "\n",
    "Because your firm is quite large, there must be enough trading volume (average amount of shares transacted per day) so that it can easily transact in these stocks. Otherwise, this effect compounded with the stocks' naturally high volatility could make these too risky for the bank to invest in. A stock with higher trading volume is easier to buy or sell quickly, which is extremely important in a volatile market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZiUta6AmWyG"
   },
   "source": [
    "**Business Problem.** Given that both low trading volume and high volatility present risks to your investments, your team lead asks you to investigate the following: **\"How is the volatility of energy stocks related to their average daily trading volume?\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RI5qxe0umWyJ"
   },
   "source": [
    "**Analytical Context.** The data you've been given is in the [Comma Separated Value (CSV) format](https://frictionlessdata.io/docs//), and comprises price and trading volume data for the above stocks. This case begins with a brief overview of this data, after which you will: (1) learn how to use the Python library [pandas](https://towardsdatascience.com/a-quick-introduction-to-the-pandas-python-library-f1b678f34673) to load the data; (2) use ```pandas``` transform this data into a form amenable for analysis; and finally (3) use ```pandas``` to analyze the above question and come to a conclusion. As you may have guessed, ```pandas``` is an enormously useful library for data analysis and manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ef8dDTzmWyL",
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "## Importing packages to aid in data analysis\n",
    "\n",
    "[External libraries (a.k.a. packages)](https://www.learnpython.org/en/Modules_and_Packages) are code bases that contain a variety of pre-written functions and tools. This allows you to perform a variety of complex tasks in Python without having to \"reinvent the wheel\" build everything from the ground up. We will use two core packages: ```pandas``` and ```numpy```.\n",
    "\n",
    "```pandas``` is an external library that provides functionality for data analysis. Pandas specifically offers a variety of data structures and data manipulation methods that allow you to perform complex tasks with simple, one-line commands.\n",
    "\n",
    "```numpy``` is a package that we will use later in the case that offers numerous mathematical operations. Together, [pandas](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v1.0.0.html) and [numpy](https://numpy.org/) allow you to create a data science workflow within Python. `numpy` is in many ways foundational to `pandas`, providing vectorized operations, while `pandas` provides higher level abstractions built on top of `numpy`.</font>\n",
    "\n",
    "Let's import both packages using the ```import``` keyword. We will rename ```pandas``` to ```pd``` and ```numpy``` to ```np``` using the ```as``` keyword. This allows us to use the short name abbreviation when we want to reference any function that is inside either package. The abbreviations we chose are standard across the data science industry and should be followed unless there is a very good reason not to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1249,
     "status": "ok",
     "timestamp": 1595517117677,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "pmZ3FFlAmWyN"
   },
   "outputs": [],
   "source": [
    "# Import the Pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# Import the NumPy package\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GsIYEBLimWyb"
   },
   "source": [
    "Now that these packages are loaded into Python, we can use their contents. Let's first take a look at ```pandas``` as it has a variety of features we will use to load and analyze our stock data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "donzIkxRmWye",
    "tags": [
     "8_min"
    ]
   },
   "source": [
    "## Fundamentals of ```pandas```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "IhTGL_zSmWyg"
   },
   "source": [
    "`pandas` is a Python library that facilitates a wide range of data analysis and manipulation. Before, you saw basic data structures in Python such as lists and dictionaries. While you can build a basic data table (similar to an Excel spreadsheet) using nested lists in Python, they get quite difficult to work with. By contrast, in `pandas` the table data structure, known as a `DataFrame`, is a first-class citizen and you can easily manipulate your data thinking of it in rows and columns.\n",
    "\n",
    "If you've ever used or heard of R or SQL before, `pandas` brings some functionality from each of these to Python, allowing you to structure and filter data more efficiently than pure Python. This efficiency is seen in two distinct ways:\n",
    "\n",
    "* Scripts written using `pandas` will often run faster than scripts written in pure Python\n",
    "* Scripts written using `pandas` will often contain far fewer lines of code than the equivalent script written in pure Python.\n",
    "\n",
    "At the core of the ```pandas``` library are two fundamental data structures/objects:\n",
    "1. [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)\n",
    "2. [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)\n",
    "\n",
    "A ```Series``` object stores single-column data along with an **index**. An index is just a way of \"numbering\" the ```Series``` object. For example, in this case study, the indices will be dates, while the single-column data may be stock prices or daily trading volume.\n",
    "\n",
    "A ```DataFrame``` object is a two-dimensional tabular data structure with labeled axes. It is conceptually helpful to think of a DataFrame object as a collection of Series objects. Namely, think of each column in a DataFrame as a single Series object, where each of these Series objects shares a common index -  the index of the DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUm0J3uWmscM"
   },
   "source": [
    "The following diagrams show the differences between a Series and a DataFrame. This is a DataFrame (notice that it has an index for its columns and an index for its rows):\n",
    "\n",
    "<img src=\"data/images/dataframe.jpg\" alt=\"A Dataframe\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-QMD7sem00L"
   },
   "source": [
    "If you *slice* a DataFrame to extract just **one** column, you'll get a Series. Series can be created independently as well. This is a Series (Series objects always have a row index):\n",
    "\n",
    "<img src=\"data/images/series.jpg\" alt=\"A Series\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YtXk59lJq9Le"
   },
   "source": [
    "Series don't have column indexes, because they don't have columns. You can, however, give your Series a name. Below is the syntax for creating a Series object, followed by the syntax for creating a DataFrame object. Note that DataFrame objects can also have a single-column – think of this as a DataFrame consisting of a single Series object (they might contain the same information, but are still distinct objects and are treated as such by `pandas`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1212,
     "status": "ok",
     "timestamp": 1595517117681,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "fhJDGHpXmWyi",
    "outputId": "c42a48ea-7cb7-465f-8495-2003b3a10b55"
   },
   "outputs": [],
   "source": [
    "# Create a simple Series object\n",
    "simple_series = pd.Series(\n",
    "    index=[0, 1, 2, 3], # The index\n",
    "    name=\"Volume\", # The name\n",
    "    data=[1000, 2600, 1524, 98000] # The data\n",
    ")\n",
    "simple_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24OVmcn7sMVg"
   },
   "source": [
    "You'll notice that this Series is of `dtype` (data type) `int64`, which means its data cells contain integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M4VMnunSmWyt"
   },
   "source": [
    "By changing ```pd.Series``` to ```pd.DataFrame```, and adding a columns input list, a DataFrame object can be created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1184,
     "status": "ok",
     "timestamp": 1595517117683,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "TPez_NTtmWyv",
    "outputId": "109b7984-2c38-4760-af1c-a98caf9b9a3b"
   },
   "outputs": [],
   "source": [
    "# Create a simple DataFrame object\n",
    "simple_df = pd.DataFrame(\n",
    "    index=[0, 1, 2, 3], # The index\n",
    "    columns=[\"Volume\"], # Your columns (must be inside a list object). AKA your column index.\n",
    "    data=[1000, 2600, 1524, 98000] # Your data\n",
    ")\n",
    "simple_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2eoDKUCmWy7"
   },
   "source": [
    "DataFrame objects are more general than Series objects, and one DataFrame can hold many Series objects, each as a different column. Let's create a two-column DataFrame object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1154,
     "status": "ok",
     "timestamp": 1595517117685,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "3vX5lIGZmWy9",
    "outputId": "64016a33-d01f-408c-8d37-4e68f002ac36",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create another DataFrame object\n",
    "another_df = pd.DataFrame(\n",
    "    index=[0, 1, 2, 3], # The index\n",
    "    columns=[\"Date\", \"Volume\"], # Your columns (again, inside a list object). We will have 2 columns.\n",
    "    data=[\n",
    "          [20190101, 1000], # The data cells for the first row (row 0.) The cell for \"Date\" and then the cell for \"Volume\"\n",
    "          [20190102, 2600], # Data cells for row 1\n",
    "          [20190103, 1524], # Data cells for row 2\n",
    "          [20190104, 98000] # Data cells for row 3\n",
    "          ],\n",
    ")\n",
    "another_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZxHPlxGJmWzL"
   },
   "source": [
    "Notice how a list of lists was used to specify the data in the ```another_df``` DataFrame. Each element of the list corresponds to a row in the DataFrame, so the list has 4 elements because there are 4 row indices. Each element of the list of lists has 2 elements because the DataFrame has two columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nUjN52gmWzN",
    "tags": [
     "10_min"
    ]
   },
   "source": [
    "## Using <code>pandas</code> to analyze stock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "71V0_AlomWzQ"
   },
   "source": [
    "Recall that we have CSV files that include data for each of the following stocks:\n",
    "\n",
    "1. Dominion Energy Inc. (Stock Symbol: D)\n",
    "2. Exelon Corp. (Stock Symbol: EXC)\n",
    "3. NextEra Energy Inc. (Stock Symbol: NEE)\n",
    "4. Southern Co. (Stock Symbol: SO)\n",
    "5. Duke Energy Corp. (Stock Symbol: DUK)\n",
    "\n",
    "The available data for each stock includes:\n",
    "\n",
    "1. **Date:** The day of the year\n",
    "2. **Open:** The stock opening price of the day\n",
    "3. **High:** The highest observed stock price of the day\n",
    "4. **Low:** The lowest observed stock price of the day\n",
    "5. **Close:** The stock closing price of the day\n",
    "6. **Adj Close:** The adjusted stock closing price for the day (adjusted for splits and dividends)\n",
    "7. **Volume:** The volume of the stock traded over the day\n",
    "\n",
    "To get a better sense of the available data, let's first take a look at just the data for Dominion Energy, listed on the New York Stock Exchange under the symbol D. You are given a CSV file that contains the company's stock data, ```D```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfs95km3vWXy"
   },
   "source": [
    "Recall that CSV files store tables using one CSV row per table row and separate the columns using commas (although other characters are possible, like semicolons):\n",
    "\n",
    "<img src=\"data/images/csv.jpg\" alt=\"A CSV file\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J18lMXYvUd5"
   },
   "source": [
    "`pandas` allows easy loading of CSV files through the use of the method [pd.read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1535,
     "status": "ok",
     "timestamp": 1595517161081,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "k5c7RO7dmWzU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load a file as a DataFrame and assign to the variable name df\n",
    "df = pd.read_csv(\"data/D.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNPKyOeqmWzh"
   },
   "source": [
    "The contents of the file ```D.csv``` are now stored in the DataFrame object ```df```.\n",
    "\n",
    "There are several common methods and attributes available to take a peek at the data and get a sense of it:\n",
    "\n",
    "1. ```DataFrame.head()```  -> returns the column names and first 5 rows by default\n",
    "2. ```DataFrame.tail()```  -> returns the column names and last 5 rows by default\n",
    "3. ```DataFrame.shape```   -> returns (num_rows, num_columns)\n",
    "4. ```DataFrame.columns``` -> returns index of columns\n",
    "5. ```DataFrame.index```   -> returns index of rows\n",
    "\n",
    "In your spare time please check the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/) and explore the parameters of these methods as well as other methods. Familiarity with this library will dramatically improve your productivity as a data scientist.\n",
    "\n",
    "Using ```df.head()``` and ```df.tail()``` we can take a look at the data contents. Unless specified otherwise, Series and DataFrame objects have indices starting at 0 and increase monotonically upward along the integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 554,
     "status": "ok",
     "timestamp": 1595517165563,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "v_wpVnsjmWzi",
    "outputId": "1a7c5ce9-a4c0-4693-c56a-eeeb8d496377"
   },
   "outputs": [],
   "source": [
    "# Look at the head of the DataFrame (i.e. the top rows of the DataFrame)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 700,
     "status": "ok",
     "timestamp": 1595517168721,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "p1ADsMMrmWzs",
    "outputId": "ae21824b-1a9d-4126-db67-a9589b00a60f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the tail of the DataFrame (i.e. the last rows of the DataFrame)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VzZrMgYrmWz2"
   },
   "source": [
    "Thus, we see there are 1259 data entries for Dominion Energy (remember that by default `pandas` indexes start with zero), each with 7 data points. The shape of a DataFrame is accessed using the ```shape``` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2623,
     "status": "ok",
     "timestamp": 1595517178200,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "NpT8gm8wmWz4",
    "outputId": "822680dd-f297-43d6-acbe-1a85ed48c84c"
   },
   "outputs": [],
   "source": [
    "# Determine the shape of the two-dimensional structure, that is (num_rows, num_columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GEqBoMJemW0E"
   },
   "source": [
    "It's important to note that ```DataFrame.columns``` and ```DataFrame.index``` return an index object instead of a list. These are, in fact, the column index and the row index of your DataFrame, respectively. To cast an index to a list for further list manipulation, we use the ```list()``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3437,
     "status": "ok",
     "timestamp": 1595517179056,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "XwO6f6ZvmW0F",
    "outputId": "4157e798-d74c-461f-bade-7caed302eb8f"
   },
   "outputs": [],
   "source": [
    "# List of the column names of the DataFrame\n",
    "list(df.columns)\n",
    "# Run df.columns alone instead and see how the output differs. This time, the result would be an \"Index\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3423,
     "status": "ok",
     "timestamp": 1595517179079,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "MwBpkzIPmW0R",
    "outputId": "8529d26e-0052-418b-85c2-247d499d07eb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of the row indexes of the DataFrame\n",
    "list(df.index)[0:20]  # only showing first 20 index values to reduce screen output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JY0rfXMmW0b",
    "tags": [
     "7_min"
    ]
   },
   "source": [
    "## Creating additional variables relevant to stock volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdk8YMFkmW0d"
   },
   "source": [
    "Oftentimes, the data provided to you will not be sufficient to achieve your goal. You may have to add additional variables or data features to assist you. Recall that our original question concerned the relationship between stock trading volume and volatility. Therefore, our DataFrame must have features related to both of these quantities.\n",
    "\n",
    "It can be helpful to think about adding columns to DataFrames as adding adjacent columns one-by-one in Excel. Here is an example of how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3393,
     "status": "ok",
     "timestamp": 1595517179086,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "29w0FEOhmW0f",
    "outputId": "f1a5ca11-26eb-4e7b-86d8-a37f892d4144"
   },
   "outputs": [],
   "source": [
    "# Add a new column named \"Symbol\"\n",
    "df[\"Symbol\"] = \"D\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3360,
     "status": "ok",
     "timestamp": 1595517179088,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "aBV8KdIXmW0v",
    "outputId": "4810325d-edaa-446b-9c84-89e24884b24c"
   },
   "outputs": [],
   "source": [
    "# We can access a column by using [] brackets and the column name\n",
    "df[\"Volume\"].head() # added .head() to suppress output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3315,
     "status": "ok",
     "timestamp": 1595517179099,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "o6Plvjn9mW07",
    "outputId": "e7102f0c-8e47-4c7c-9dc7-ca8fd747ced4"
   },
   "outputs": [],
   "source": [
    "# Add a new column named \"Volume_Millions\", which is calculated from the Volume column currently in df\n",
    "# divide every row in df['Volume'] by 1 million, store in new column\n",
    "df[\"Volume_Millions\"] = df[\"Volume\"] / 1000000\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3271,
     "status": "ok",
     "timestamp": 1595517179103,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "NAKQi3L0mW1K",
    "outputId": "f5f9d3e3-f884-4d94-f374-2411399887dd"
   },
   "outputs": [],
   "source": [
    "# Take a look at the updated DataFrame shape. Two new columns have been added.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fv8T3-FhmW1W"
   },
   "source": [
    "As discussed, we need to have a feature in our DataFrame that is related to volatility. Because this currently does not exist, we must create it from the already available features. Recall that volatility is a measure of the variability of daily returns over a period of time, so let's create a feature for daily returns. Daily returns are defined as the percentage gain or loss of a stock when comparing today's close price to the previous day's close price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3272,
     "status": "ok",
     "timestamp": 1595517179113,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "U-d-WeDzkONa"
   },
   "outputs": [],
   "source": [
    "# Daily returns are the percentage change between today's close and the previous day's close\n",
    "# To get the previous day's close, we use Series.shift(1) (this lags our series by one day)\n",
    "df[\"Return\"] = (df[\"Close\"] / df[\"Close\"].shift(1)) - 1.0 # We subtract 1 to make this a growth percentage. So, for instance, not 1.2 (120%) but 0.2 (20%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kvppdKwlkiuh"
   },
   "source": [
    "Let's create another statistical quantity to measure our volatility. Let's call it `VolStat`. This quantity takes the range of the price of each day (`High` - `Low`) rather than the difference between today's closing price and yesterday's closing price. This metric captures movements in the middle of each day, rather than just movements between days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3272,
     "status": "ok",
     "timestamp": 1595517179120,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "4gLIi53dmW1Y"
   },
   "outputs": [],
   "source": [
    "df[\"VolStat\"] = (df[\"High\"] - df[\"Low\"]) / df[\"Open\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdn77MPYmW1g"
   },
   "source": [
    "Here we see the power of ```pandas```. We can simply perform mathematical operations on columns of DataFrames just as if the DataFrames were single variables themselves. Let's check the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3238,
     "status": "ok",
     "timestamp": 1595517179122,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "xytUJHjvmW1i",
    "outputId": "878f6ad5-bd6d-418b-b6c9-ea8aa8bfa939"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UaV8XitRmW1r"
   },
   "source": [
    "Now we have features relevant to the original question, and can proceed to the analysis step. A common first step in data analysis is to learn about the **distribution** of the available data. The distribution refers to how the data is dispersed across all possible values that the data could take on. This will be made clear next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ud4UxjUhmW1t",
    "tags": [
     "30_min"
    ]
   },
   "source": [
    "## Learning about the data distribution through summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIt96L-_7oMN"
   },
   "source": [
    "Let's take a simple example - suppose you have a list of ages taken from a survey you conducted among your friends. The ages are:\n",
    "\n",
    "```\n",
    "21, 30, 35, 40, 41, 45, 44, 44, 48, 43, 70\n",
    "```\n",
    "\n",
    "If you needed to summarize this set of numbers, you could group them in subsets (by tens for example; e.g. 20s, 30s, 40s, etc.) and count the number of numbers within each subset. In this example, the resulting *frequency table* would be:\n",
    "\n",
    "| Group | Number of people |\n",
    "| ------| -----------------|\n",
    "20-29 | 1 |\n",
    "30-39 | 2 |\n",
    "40-49 | 7 |\n",
    "50-59 | 0 |\n",
    "60-69 | 0 |\n",
    "70-79 | 1 |\n",
    "\n",
    "So, instead of reciting the ages of your friends one-by-one every time someone asks you about them, you can simply report the frequency table and say \"most of my friends are between 40 and 49 years old\". When you make claims like this, you are describing the *distribution* of the available data across the subsets you have defined.\n",
    "\n",
    "`pandas` has convenient functionality that allows you to easily create frequency tables and even plot them. Let's plot the distribution of `Volume_Millions`. We can use `pd.cut()` to calculate our frequency table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3207,
     "status": "ok",
     "timestamp": 1595517179124,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "FWK3TLig_bVm",
    "outputId": "a17299a1-c21c-42fa-ba2d-034d8fc45248"
   },
   "outputs": [],
   "source": [
    "# Let's define our bins (the boundaries of the subgroups)\n",
    "bins = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15] # These are given in millions of shares\n",
    "freq  = pd.cut(df[\"Volume_Millions\"], bins=bins) # This \"cuts\" the Volume_Millions Series into the bins\n",
    "freq = freq.value_counts(sort=False) # Now we count the number of elements inside each bin. We use sort=False to avoid having our groups sorted by size\n",
    "freq # The result is our frequency table. Freq is actually a Series in which the index is the bins and the data are the counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qCDXwhUOBhkI"
   },
   "source": [
    "And then make a **histogram** of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3467,
     "status": "ok",
     "timestamp": 1595517179416,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "I0YSBJoWBImk",
    "outputId": "77d95bcb-b37b-4522-f9b5-155151a8771d"
   },
   "outputs": [],
   "source": [
    "freq.plot.bar() # Since freq is a Series, we can use .plot.bar() to easily create a barplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJoK8F0MBpjc"
   },
   "source": [
    "A histogram shows how many values in a particular dataset fall into any of several pre-defined \"buckets\". In this case, \"buckets\" represent the volume in millions and are. Instead of writing all the code above, you can alternatively use this nice one-liner:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3833,
     "status": "ok",
     "timestamp": 1595517179815,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "I_bxxiZ6_FSf",
    "outputId": "c47028ef-1605-4163-df8b-8fc312dd2871"
   },
   "outputs": [],
   "source": [
    "# This one doesn't have white space between the bars, which makes sense because the X axis does not have gaps!\n",
    "# Since df[\"Volume_Millions\"] is a series as well, we can use .plot.hist() to create a histogram.\n",
    "df[\"Volume_Millions\"].plot.hist(bins=bins) # Pandas computes the frequency table under the hood and plots it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGKLrddBC49n"
   },
   "source": [
    "We can see that on most days, the traded volume is higher than 1 million but lower than 4 million. There are very few days with traded volume higher than 6 million."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arwRlZVXEdJT"
   },
   "source": [
    "While creating frequency tables and plotting them is useful, we sometimes want to have an even more summarized view of our data. For that, we compute **summary statistics**. You can think of your histogram is a summary of your raw data, and your summary statistics as a summary of the shape of your histogram.\n",
    "\n",
    "Let's aggregate summary statistics for the five energy sector companies under study. Fortunately, the DataFrame and Series objects offer a myriad of data summary statistics methods:\n",
    "\n",
    "1. ```min()```\n",
    "2. ```max()```\n",
    "3. ```mean()```\n",
    "4. ```quantile()```\n",
    "5. ```median()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rR0PXBZ0o_Pk"
   },
   "source": [
    "We can visualize the first three quantities using an example histogram (the underlying dataset is simply a bunch of random numbers):</font>\n",
    "\n",
    "<img src=\"data/images/hist_min_max_mean.jpg\" alt=\"Some summary statistics\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0aXtv-7vIt1"
   },
   "source": [
    "`min` tells you what the minimum value in your dataset is, and `max` tells you what the maximum value is. The **[mean](https://www.mathgoodies.com/lessons/vol8/mean)** is a type of average that attempts to report the \"center\" of your distribution; i.e. the \"center\" of your histogram. If your data are mostly concentrated around the middle of the histogram (like in this example), then the mean is a very good summary of the \"center\" of the dataset; however, as you will see, this is not always the case.\n",
    "\n",
    "Let's now compute and plot summary statistics for `Volume_Millions`. Notice how simple the functions are to apply to the DataFrame. Simply type the name of the DataFrame, followed by a ```.``` and then the method name you'd like to calculate. We've chosen to select a single column ```Volume_Millions``` from the DataFrame ```df```, but you could have just as easily called these methods on the full DataFrame rather than a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1595520851525,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "S0n1DqAsmW1x",
    "outputId": "ea48806e-7156-4dd7-db09-e50d23697734"
   },
   "outputs": [],
   "source": [
    "# Calculate the minimum of the Volume_Millions column\n",
    "df[\"Volume_Millions\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1595520860560,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "dcFoq1SB1rBs",
    "outputId": "61dd9ea5-9c59-42e5-8f25-6ba3409b3b09"
   },
   "outputs": [],
   "source": [
    "# Calculate the maximum of the Volume_Millions column\n",
    "df[\"Volume_Millions\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JiIZQaQf17mn"
   },
   "source": [
    "For the mean, we also plot the accompanying histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4280,
     "status": "ok",
     "timestamp": 1595517180434,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "Z1IWnmaR18oT",
    "outputId": "ef045106-d1a2-4b15-eaaf-a761a5924a91"
   },
   "outputs": [],
   "source": [
    "# Calculate the mean of the Volume_Millions column\n",
    "the_mean = df[\"Volume_Millions\"].mean()\n",
    "ax = df[\"Volume_Millions\"].plot.hist(bins=bins) # We save our plot to a variable called \"ax\"\n",
    "ax.axvline(x=the_mean, color=\"red\") # And then use the axvline method to add a vertical line at a desired point on the X axis\n",
    "ax.text(x=5, y=450, s=\"The mean value is \" + str(the_mean)) # We can add some text too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvuau2ib2q9-"
   },
   "source": [
    "The mean attempts to describe where the \"center\" of the histogram is, with the hope that most (or at least a large chunk) of the data lies around that \"center\". Now, let's look at the **median** value of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1595521521694,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "pe4GR5wA2to9",
    "outputId": "08625dca-ada9-4308-c3c8-0a109b4b00c6"
   },
   "outputs": [],
   "source": [
    "# Calculate the mean of the Volume_Millions column\n",
    "the_mean = df[\"Volume_Millions\"].mean()\n",
    "the_mean = round(the_mean, 2) # Let's round it to make it more readable\n",
    "\n",
    "# Calculate the median of the Volume_Millions column\n",
    "the_median = df[\"Volume_Millions\"].median()\n",
    "the_median = round(the_median, 2) # Let's round it to make it more readable\n",
    "\n",
    "ax = df[\"Volume_Millions\"].plot.hist(bins=bins)\n",
    "ax.axvline(x=the_mean, color=\"red\")\n",
    "ax.axvline(x=the_median, color=\"green\")\n",
    "ax.text(x=5, y=450, s=\"The mean value is \" + str(the_mean), color=\"red\")\n",
    "ax.text(x=5, y=350, s=\"The median value is \" + str(the_median), color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-ncmkDQ3Asw"
   },
   "source": [
    "Unlike the mean, the median gives the 50th percentile value in your dataset; i.e. the number such that half of the values in the dataset lie below it, and half of the values lie above it. When your distribution is symmetrical, your mean and median usually match. But when it is asymmetrical, like this one, they generally don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "### Question:\n",
    "\n",
    "In this scenario, does the mean or the median give a better summary of the \"typical\" value in the dataset? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bvb74F6U4cLI"
   },
   "source": [
    "Finally, let's compute the `quantile` statistics. The `quantile()` function gives you the $X$-th percentile of your dataset; i.e. the value such that $X$ percent of your dataset is less than that value. The median is a classical case of this, where $X = 50$. As another example, a cutoff of $X = 25$ means that 25\\% of your data are below the resulting value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 940,
     "status": "ok",
     "timestamp": 1595525401733,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "OIb3FtGl5nFv",
    "outputId": "d5be24f1-276e-402a-8d22-43e9474eecd7"
   },
   "outputs": [],
   "source": [
    "# Calculate the 25% quantile of the Volume_Millions column\n",
    "the_25 = df[\"Volume_Millions\"].quantile(0.25)\n",
    "ax = df[\"Volume_Millions\"].plot.hist(bins=bins)\n",
    "ax.axvline(x=the_25, color=\"red\")\n",
    "ax.text(x=5, y=450, s=\"The 25% quantile is \" + str(the_25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-gb5xJm6oBZ",
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Write code to calculate and plot the 50\\%, the 75\\% and the 100\\% quantiles. You'll see that the 50\\% quantile and the *median* are one and the same, by definition. Also, the 100\\% quantile is the same as the *maximum*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCiP4-z0vPwe",
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_IBpkvmmW3i"
   },
   "source": [
    "Is there a more efficient method to quickly compute all of these summary statistics? Yes. One incredibly useful method that combines these summary statistics and also adds a couple others is the [describe()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4795,
     "status": "ok",
     "timestamp": 1595517181111,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "kOGde1IWmW3k",
    "outputId": "7bd38ecd-ffd6-4c72-e816-04976e070759"
   },
   "outputs": [],
   "source": [
    "df['Volume_Millions'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWrB9sNtmW3y"
   },
   "source": [
    "From this distribution analysis of the daily trading volume, we can see that more than 14 million shares would be a very large trading day, whereas below 2 million shares would be a relatively small trading day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Rhc7j4mmW30"
   },
   "source": [
    "In addition to describe, there is a [value_counts() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) for checking the frequency of elements in categorical data. Please be aware that `value_counts()` is a method of the Series class and NOT the DataFrame class. This means you have to isolate a specific column of a DataFrame before calling `value_counts()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4763,
     "status": "ok",
     "timestamp": 1595517181113,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "bCgrCwUsmW33",
    "outputId": "63386a3e-69b1-446b-fe48-1f1f3a7ecfaa"
   },
   "outputs": [],
   "source": [
    "dict_data = {\n",
    "    \"numbers\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    \"color\": [\"red\", \"red\", \"red\", \"blue\", \"blue\", \"green\", \"blue\", \"green\"],\n",
    "}\n",
    "category_df = pd.DataFrame(data=dict_data)\n",
    "\n",
    "category_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4756,
     "status": "ok",
     "timestamp": 1595517181114,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "QlH0HVkbmW4M"
   },
   "outputs": [],
   "source": [
    "#why doesn't this work? (uncomment the expression that follows)\n",
    "#category_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4751,
     "status": "ok",
     "timestamp": 1595517181116,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "0C5iO04nmW4x"
   },
   "outputs": [],
   "source": [
    "# only Series objects can call this method (uncomment the following expression)\n",
    "# category_df['color'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsOngwFmmW5B",
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Determine the 25th, 50th, and 75th percentile for the ```Open```, ```High```, ```Low```, and ```Close``` columns of ```df```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vKPdNA3smW5H",
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TvyCyGMHmW6E",
    "tags": [
     "25_min"
    ]
   },
   "source": [
    "## Aggregating data from multiple companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "g0wHhSqemW6H"
   },
   "source": [
    "So far, we've only been looking at data from one of our five companies. Let's go ahead and combine all five CSV files to analyze the five companies together. This will also reduce the amount of programming work required since the code will be shared across the five companies.\n",
    "\n",
    "One way to accomplish this aggregation task is to use the [pd.concat()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) method from ```pandas```. An input into this method may be a list of DataFrames that you'd like to concatenate. We will use a `for` loop to loop over each stock symbol, load the corresponding CSV file, and then append the result to a list which is later aggregated using ```pd.concat()```. Let's take a look at how this is done. We want to:\n",
    "\n",
    "1. Load all the CSV files into `pandas` DataFrames.\n",
    "2. Add a new column to each one of them with the respective symbol name to distinguish between stocks in the final DataFrame.\n",
    "3. Combine all the DataFrames into one big DataFrame (we'll be using `pd.concat()` for that).\n",
    "4. Create the `Volume_Millions`, `VolStat` and `Return` features in the concatenated DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSbm5y_jtymA"
   },
   "source": [
    "We can use a `for` loop to perform steps 1 and 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5542,
     "status": "ok",
     "timestamp": 1595517181962,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "k-WnjDL9t333"
   },
   "outputs": [],
   "source": [
    "# Defining our stock symbols\n",
    "symbol_data_to_load = [\"D\", \"EXC\", \"NEE\", \"SO\", \"DUK\"]\n",
    "list_of_df = [] # An empty list that we'll be populating iteratively\n",
    "\n",
    "# Looping over the symbols to load the dataframes\n",
    "for symbol in symbol_data_to_load:\n",
    "    # Load the CSV file\n",
    "    temp_df = pd.read_csv(\"data/\" + symbol + \".csv\")\n",
    "    \n",
    "    # Add new column with symbol name to distinguish in final dataframe\n",
    "    temp_df[\"Symbol\"] = symbol\n",
    "    list_of_df.append(temp_df) # Append the loaded DataFrame to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ScbT5yyuYIK"
   },
   "source": [
    "With this, we now have a list (`list_of_df`) that contains all of our loaded DataFrames. We can now concatenate the DataFrames very easily with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5536,
     "status": "ok",
     "timestamp": 1595517181964,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "YFUdNQnOujer"
   },
   "outputs": [],
   "source": [
    "agg_df = pd.concat(list_of_df, axis=0) # Axis 0 means we want to pile them, not glue them together horizontally (that would be axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dnr-zON9vR5g"
   },
   "source": [
    "Let's now create the `Volume_Millions`, `VolStat` and `Return` features in the concatenated DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5530,
     "status": "ok",
     "timestamp": 1595517181966,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "i3ZNA5EcvcxI"
   },
   "outputs": [],
   "source": [
    "agg_df[\"Volume_Millions\"] = agg_df[\"Volume\"] / 1000000.0\n",
    "agg_df[\"VolStat\"] = (agg_df[\"High\"] - agg_df[\"Low\"]) / agg_df[\"Open\"]\n",
    "agg_df[\"Return\"] = (agg_df[\"Close\"] / agg_df[\"Close\"].shift(1)) - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tS08sAcVvjLY"
   },
   "source": [
    "We can see both the tail and the shape of the result below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5504,
     "status": "ok",
     "timestamp": 1595517181969,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "Co64fRCJmW6I",
    "outputId": "d4b73d41-9f58-4a29-a595-fc5f0752fb44"
   },
   "outputs": [],
   "source": [
    "agg_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5383,
     "status": "ok",
     "timestamp": 1595517181971,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "7uWyhuqOw-Bv",
    "outputId": "876dc956-597f-44dd-8d23-a207d41a49ec"
   },
   "outputs": [],
   "source": [
    "agg_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Orzd_lR_mW6v"
   },
   "source": [
    "After the `for` loop, we've aggregated and added the relevant features we identified in the previous section. We then printed the tail of the aggregated DataFrame to have a peek at the format of the data, and we've also printed the shape of the DataFrame. This is to sanity-check that our final DataFrame is roughly what we expect. Notice the aggregated DataFrame has the same number of columns as the original single stock (D) data, however the number of rows have increased five-fold. This makes sense, because each additional symbol contains 1259 data entries, so five symbols leads to a total of ```1259*5 = 6295``` rows. So, this passes our sanity check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQArRtXOmW61"
   },
   "source": [
    "Now, if we want to reverse this process and extract the data relevant to a single stock symbol from the aggregated DataFrame ```agg_df```, we can do so using the ```==``` operator, which returns True when two objects contain the same value, and False otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5334,
     "status": "ok",
     "timestamp": 1595517181973,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "si3NFrlOxjjo",
    "outputId": "cc425ebf-140a-4bff-c3f8-5b02a59e496d"
   },
   "outputs": [],
   "source": [
    "# A column of Trues and Falses (also known as a \"Boolean\" column). True if the symbol is DUK, false otherwise\n",
    "are_you_DUK = agg_df[\"Symbol\"] == \"DUK\"\n",
    "are_you_DUK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5289,
     "status": "ok",
     "timestamp": 1595517181974,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "a-xg_o_7mW6_",
    "outputId": "d7c08fbe-cb19-4d60-ccc0-736d94bca1ce"
   },
   "outputs": [],
   "source": [
    "# Filtering our concatenated dataframe using the filtering column\n",
    "symbol_DUK_df = agg_df[are_you_DUK] # We tell Pandas to only include the rows for which are_you_DUK is True.\n",
    "symbol_DUK_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjEtZJtpmW7X"
   },
   "source": [
    "This row extraction technique will be useful to us later in this case when we perform analyses on each individual stock symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C36o8fHQmW7Y",
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "### Exercise 3:\n",
    "\n",
    "If we instead used the ```!=``` operator in the definition of the filtering column, how many rows would we end up with?\n",
    "\n",
    "(a) 1259\n",
    "\n",
    "(b) 12590\n",
    "\n",
    "(c) 5036\n",
    "\n",
    "(d) 6295"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eNHWUpHUmW7a",
    "tags": [
     "12_min"
    ]
   },
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Write a `for` loop that loops through each of the five symbols, extracting only the rows corresponding to each symbol, and prints the average ```VolStat``` value for each of the five symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0RrLAJFmW7b",
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHXN99hBp5U0"
   },
   "source": [
    "It turns out that we can use an alternative approach to reduce the amount of code we need to write, especially if we plan on repeating this procedure in the future. We can define a Python function to do the filtering and the average calculation and then pass it to the `for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5457,
     "status": "ok",
     "timestamp": 1595517182199,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "839IHrhhqNP3"
   },
   "outputs": [],
   "source": [
    "def do_some_stuff(symbol):\n",
    "    condition = agg_df[\"Symbol\"] == symbol # We create a boolean column to filter the DataFrame\n",
    "    symbol_df = agg_df[condition] # We filter the DataFrame with our condition column\n",
    "    symbol_avg_volstat = symbol_df[\"VolStat\"].mean() # Finding the mean\n",
    "    return [symbol, symbol_avg_volstat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5408,
     "status": "ok",
     "timestamp": 1595517182203,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "-Vg6Qc68qVXR",
    "outputId": "8d0a77c9-f7ca-4095-cef2-9ff5c707a46d"
   },
   "outputs": [],
   "source": [
    "my_list = [do_some_stuff(symbol) for symbol in symbol_list]\n",
    "my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtnOWGs7qiv_"
   },
   "source": [
    "You might be wondering what kind of expression the `for` loop above is. It's a `for` loop, for sure, but the syntax seems *wrong*! But it's actually perfectly legal - this is known as **list comprehension**, which allows us to run the `for` loop and create a list of the results, *all at once*. Notice that `my_list` is a list of lists because the output of `do_some_stuff()` is itself a list that contains the symbol and the average `VolStat`. The more verbose equivalent of this list comprehension would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5373,
     "status": "ok",
     "timestamp": 1595517182206,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "iWu1lRrzskr8",
    "outputId": "6cdf3e56-ad54-4e07-fb7f-c6202772649b"
   },
   "outputs": [],
   "source": [
    "my_list = [] # We create an empty list\n",
    "for symbol in symbol_list:\n",
    "    my_list.append(do_some_stuff(symbol)) # We execute the custom function and iteratively append its result to our list\n",
    "\n",
    "my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9EpZ12ItTeC"
   },
   "source": [
    "The syntax for a list comprehension is always `[expression for item in list]`, as explained [here](https://www.programiz.com/python-programming/list-comprehension). In our example, the expression was the function `do_some_stuff`, the item was `symbol` and the original list was `symbol_list`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axleSiIGtyGj"
   },
   "source": [
    "You can use list comprehension even if you don't want to define a custom function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5342,
     "status": "ok",
     "timestamp": 1595517182208,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "NxOb-StUuHRZ",
    "outputId": "2d1422d0-dc39-4b1a-9922-aecc20d450e3"
   },
   "outputs": [],
   "source": [
    "my_list = [\"This is a nice stock >>> \" + symbol for symbol in symbol_list]\n",
    "my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPZF-rYTvcZi",
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "### Question:\n",
    "\n",
    "How would you code a list comprehension to create a list that is an exact copy of `symbol_list`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gqnGg_gkmW7u",
    "tags": [
     "20_min"
    ]
   },
   "source": [
    "## Analyzing each stock's volatility levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "P9GyhXB2mW7w"
   },
   "source": [
    "```pandas``` offers the ability to group related rows of DataFrames according to the values of other rows. This useful feature is accomplished using the **[groupby()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)** method.  Let's take a look and see how this can be used to group rows so that each group corresponds to a single stock symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5302,
     "status": "ok",
     "timestamp": 1595517182210,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "G7_uCqC6mW7y",
    "outputId": "e1f5ce4d-6934-4cb6-ead0-7dcf866808d5"
   },
   "outputs": [],
   "source": [
    "# Use the groupby() method, notice a DataFrameGroupBy object is returned\n",
    "agg_df.groupby('Symbol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "LNaBLJxQmW77"
   },
   "source": [
    "Here, the ```DataFrameGroupBy``` object can be most readily thought of as containing a DataFrame object for every group (in this case, a DataFrame object for each symbol). Specifically, each item of the object is a tuple, containing the group identifier (in this case the `Symbol`), and the corresponding rows of the DataFrame that have that `Symbol`). You can think of the `groupby()` as a `for` loop through the values of a user-specified column and, in each iteration, extracts only those DataFrame rows that match that value in the specified column. The below diagram visualizes a `groupby()` operation over the column `foo`. Since there are only two values in `foo` (`one` and `two`), there are only two resulting groups:\n",
    "\n",
    "<img src=\"data/images/groupby.jpg\" alt=\"Groupby\" style=\"width: 500px;\"/>\n",
    "\n",
    "Source: Modified from [Pandas docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html).\n",
    "\n",
    "With ```pandas``` you can iterate over the `groupby()` object to see what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5257,
     "status": "ok",
     "timestamp": 1595517182212,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "SJIItgL2mW78",
    "outputId": "f0dc5b83-b24f-4d43-a319-b7113174b240",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grp_obj = agg_df.groupby(\"Symbol\")  # Group data in agg_df by Symbol\n",
    "\n",
    "# Define a function\n",
    "def show_me_your_contents(group):\n",
    "    symbol = group[0] # group[0] is the symbol\n",
    "    head_of_df = group[1].head() # group[1].head() is the DataFrame (only the head)\n",
    "    return [symbol, head_of_df]\n",
    "\n",
    "# Loop through groups\n",
    "\n",
    "[ show_me_your_contents(group) for group in grp_obj ] # Using list comprehension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgbYcD9_mW8F"
   },
   "source": [
    "Let's combine the ```pd.groupby()``` method with the ```describe()``` method and apply it to each symbol to analyze the distribution of volatility related features for each symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5220,
     "status": "ok",
     "timestamp": 1595517182215,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "7gniBDocmW8K",
    "outputId": "f50c2cde-8f87-47dc-c0d8-0408dd8c7344",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grp_obj = agg_df.groupby(\"Symbol\")  # Group data in agg_df by Symbol\n",
    "\n",
    "# Loop through groups\n",
    "for group in grp_obj:\n",
    "    print(\"------Symbol: \", group[0])\n",
    "    grp_df = group[1]\n",
    "    relevant_df = grp_df[[\"VolStat\"]]\n",
    "    print(relevant_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TsPCHW78mW8X"
   },
   "source": [
    "One immediate observation of note is that the volatility level on any given day can vary widely. This is evident from the wide spread between the minimum and maximum ```VolStat``` levels seen using the ```describe()``` method. For example, stock symbol D has a minimum ```VolStat``` value of 0.003640, while its maximum ```VolStat``` value is 0.062350. That's more than a 10x increase in the value of ```VolStat```!\n",
    "\n",
    "While this is great to see, there is a more powerful way to display this data in `pandas`. We can call the ```describe()``` method directly on the ```DataFrameGroupBy``` object. This one line allows you to avoid having to write a `for` loop every time you'd like to summarize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5364,
     "status": "ok",
     "timestamp": 1595517182417,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "c1tKnjT2mW8a",
    "outputId": "54b87360-4f59-4ad5-aae3-34e22ab1a657",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VolStat\n",
    "agg_df[[\"Symbol\", \"VolStat\"]].groupby(\"Symbol\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVXKQZ_emW8v"
   },
   "source": [
    "This data is identical to the data previously outputted using the `for` loop approach. The difference is that utilizing the features of the ```DataFrameGroupBy``` object allows for easy coding, fast results, and a clean output. This illustrates the power of using the ```pd.groupby()``` method: generating statistics for groups of interest in your data is straightforward and efficient to code.\n",
    "\n",
    "You'll notice this pattern a lot as you gain more familiarity with Python and data analysis. There are many ways to solve a problem, but often one way is substantially more efficient, both in terms of run time and in terms of lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_U4XD2e3mW82",
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "### Exercise 5:\n",
    "\n",
    "What are some insights you can draw from the ```VolStat``` summary statistics in terms of volatility levels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ooyJ9jHmW84",
    "tags": [
     "6_min"
    ]
   },
   "source": [
    "### Exercise 6:\n",
    "\n",
    "Using ```agg_df```, write a script to determine the mean value of ```VolStat``` for each symbol by year.\n",
    "\n",
    "**Hint:** Remember that the year can be taken from the `Date` column via `date_string[0:4]`; that is, the first four characters of the `Date` string (if you need a refresher on string slicing in Python, you can check out [this article](https://www.digitalocean.com/community/tutorials/how-to-index-and-slice-strings-in-python-3))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ni_zyrmHmW88",
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sg7dwOwAmW9K",
    "tags": [
     "7_min"
    ]
   },
   "source": [
    "## Labelling data points as high or low volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g4Zul5dqmW9O"
   },
   "source": [
    "Now that we've determined that the volatility levels of each stock can vary widely, the next logical step is to group periods of high and low volatility so that we can then look at how volume differs between those time periods.\n",
    "\n",
    "However, we don't currently have a column that identifies when volatility is high and when it is low. Therefore, we must create a new column called ```VolLevel``` using some volatility threshold. For example, we'd like to have a new column value determined by:\n",
    "\n",
    "``` python\n",
    "if VolStat > threshold:\n",
    "    VolLevel = 'HIGH'\n",
    "else:\n",
    "    VolLevel = 'LOW'\n",
    "```\n",
    "\n",
    "Here we will define low volatility levels by any ```VolStat``` below the 50th percentile (i.e. below the median). Each percentile value must be calculated by symbol to ensure that each symbol is individually analyzed.\n",
    "\n",
    "Let's take a look how we can accomplish this task using ```groupby()``` functionality and the ```quantile()``` method, which returns the percentile for a given series of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5303,
     "status": "ok",
     "timestamp": 1595517182423,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "lxICnI04mW9Q",
    "outputId": "d2f31b3f-1a7d-464c-9175-ac88d2ba8b57"
   },
   "outputs": [],
   "source": [
    "# Determine lower thresholds for volatility for each symbol\n",
    "volstat_thresholds = agg_df.groupby(\"Symbol\")[\"VolStat\"].quantile(0.5)  # 50th percentile (median)\n",
    "print(volstat_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3ZxWqV4mW9i"
   },
   "source": [
    "Since we'd like to label periods of high and low volatility by symbol, we will make use of the [np.where()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html) method in the ```numpy``` library. This method takes an input and checks a logical condition: if the condition is true, it will return its second argument, whereas if the condition is false, it will return its third argument. This is very similar to how Microsoft Excel's ```IFERROR()``` method works (helpful to think of it this way for those familiar with Excel). Let's loop through each symbol and label each day as either high or low volatility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5277,
     "status": "ok",
     "timestamp": 1595517182425,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "agApx0TemW9m",
    "outputId": "050e30f3-53ab-4580-c275-1af6cd6ae97f"
   },
   "outputs": [],
   "source": [
    "# Loop through symbols\n",
    "print(\"Defining stock symbols\")\n",
    "list_of_symbols = [\"D\", \"EXC\", \"NEE\", \"SO\", \"DUK\"]\n",
    "list_of_df = []\n",
    "\n",
    "# Loop over all symbols\n",
    "print(\" --- Loop over symbols --- \")\n",
    "for i in symbol_data_to_load:\n",
    "    print(\"Labelling Volatility regime for Symbol: \" + i)\n",
    "    temp_df = agg_df[agg_df[\"Symbol\"] == i].copy()  # make a copy of the dataframe to ensure not affecting agg_df\n",
    "    volstat_t = volstat_thresholds.loc[i] # Extracting only the threshold we're interested in\n",
    "    temp_df[\"VolLevel\"] = np.where(temp_df[\"VolStat\"] < volstat_t, \"LOW\", \"HIGH\")  # Volatility regime label\n",
    "    list_of_df.append(temp_df)\n",
    "\n",
    "print(\" --- Completed loop over symbols --- \")\n",
    "\n",
    "print(\"Aggregating data\")\n",
    "labeled_df = pd.concat(list_of_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5560,
     "status": "ok",
     "timestamp": 1595517182733,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "XZ17crwLmW9w",
    "outputId": "57d6dbf8-1aa5-4d01-9264-ebeaa0aa44bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c52TsCbymW99"
   },
   "source": [
    "We've now added a ```VolLevel``` column that identifies whether each symbol is in a period of high or low volatility on any given day. Since we know that the bank will require higher trading volume in order to transact in periods of high volatility, let's now take a look at the average daily traded volume for high volatility vs. low volatility days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ECeNb6NEmW9_",
    "tags": [
     "15_min"
    ]
   },
   "source": [
    "## Is daily trading volume affected by the level of volatility?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SIsS0rvmW-A"
   },
   "source": [
    "To explore the relationship between volatility level and daily trading volume, let's group by ```VolLevel``` and take a look at the average ```Volume_Millions``` for the high and low volatility groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5536,
     "status": "ok",
     "timestamp": 1595517182735,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "2j0YlDzlmW-C",
    "outputId": "e9b540a5-a7f3-4323-b0d1-a64bd739154c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_df.groupby([\"Symbol\", \"VolLevel\"])[[\"Volume_Millions\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HW_IyR43mW-N",
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "### Exercise 7:\n",
    "\n",
    "What is an immediate trend you notice regarding the volatility regimes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mEYQ0871mW-P",
    "tags": [
     "12_min"
    ]
   },
   "source": [
    "### Exercise 8:\n",
    "\n",
    "Write code to group time periods into low, medium, and high volatility regimes, where:\n",
    "\n",
    "``` python\n",
    "if VolStat > (75th percentile VolStat for given symbol):\n",
    "    VolLevel = 'HIGH'\n",
    "elif  VolStat > (25th percentile VolStat for given symbol):\n",
    "    VolLevel = 'MEDIUM'\n",
    "else:\n",
    "    VolLevel = 'LOW'\n",
    "```\n",
    "\n",
    "Output a ```final_df``` DataFrame output grouped by `Symbol`, showing the mean `Volume` for each `VolLevel` category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41GK3nddmW-Q",
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLDw9uS6mW-g",
    "tags": [
     "45_min"
    ]
   },
   "source": [
    "## Graphing volatility across time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlc3KAwPmW-i"
   },
   "source": [
    "We've now satisfactorily answered our original question. However, you don't need to just analyze data in tabular format. Python contains functionality to allow you to analyze your data visually as well.\n",
    "\n",
    "We will use ```pandas``` functionality built on the standard Python plotting library [matplotlib](https://matplotlib.org/). Let's import the library and instruct Jupyter to display the plots inline (i.e. display the plots to the notebook screen so we can see them as we run the code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5513,
     "status": "ok",
     "timestamp": 1595517182742,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "DTKJyBcqmW-k"
   },
   "outputs": [],
   "source": [
    "# import fundamental plotting library in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Instruct jupyter to plot in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-EuSii_nmW-x"
   },
   "source": [
    "Before we plot, we need to convert the ```Date``` column in ```agg_df``` to a ```datetime```-like object, Python's internal data representation of dates. ```pandas``` offers the [to_datetime()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) method to convert a string that represents a given date format into a ```datetime```-like object. We instruct ```pandas``` to use ```format='%Y-%m-%d'```, since our dates are in this format, where %Y indicates the numerical year, %m indicates the numerical month and %d indicates the numerical day. If our dates were in another format, we'd modify this input value appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5493,
     "status": "ok",
     "timestamp": 1595517182745,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "5__SxDMXmW-z",
    "outputId": "4a62439b-459b-4f34-eb31-d2c0fab88a91"
   },
   "outputs": [],
   "source": [
    "# To convert a string to a datetime\n",
    "agg_df[\"DateTime\"] = pd.to_datetime(agg_df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Set index as DateTime for plotting purposes\n",
    "agg_df = agg_df.set_index([\"DateTime\"])\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgFu4TP_mW-9"
   },
   "source": [
    "Now we are ready to look directly at volatility across time. Let's group by symbols and plot the ```VolStat``` value across time. Each symbol's time series will be labelled a different color by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6243,
     "status": "ok",
     "timestamp": 1595517183525,
     "user": {
      "displayName": "Norman Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiXcbIxQ0CYzDTlPuMhfh4q4RrONoW5L_HFnRCh=s64",
      "userId": "01368312674772044880"
     },
     "user_tz": 300
    },
    "id": "VCOk5yLtmW-_",
    "outputId": "c1cc2312-29c4-4a5c-baea-e90027045604"
   },
   "outputs": [],
   "source": [
    "# Look at volatility regimes\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "agg_df.groupby(\"Symbol\")[\"VolStat\"].plot(\n",
    "    ax=ax, legend=True, title=\"Energy Sector Trends - VolStat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ga30D6WKmW_K"
   },
   "source": [
    "We notice that periods of high volatility tend to \"clump\" together; that is, periods of high volatility are not uniformly and randomly distributed across time, but tend to occur in highly concentrated bursts. This is an interesting insight that we could not gain by only looking at the data in tabular format. In future cases, you will dig deeper into the numerous graphing capabilities of Python and how to integrate them into your data science workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWvlYrRymW_M",
    "tags": [
     "12_min"
    ]
   },
   "source": [
    "### Exercise 9:\n",
    "\n",
    "Write a script to find and print the month that has the highest average daily trading volume for each symbol. Also include the average volume value corresponding to that month. For example, symbol D has its highest average daily trading volume of 6.437 million in December 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LSKda93mW_O",
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "10_min"
    ]
   },
   "source": [
    "### Exercise 10:\n",
    "\n",
    "We have so far looked at volatility grouped by stock symbol or by year and month. As our data covers several years, it's also interesting to group the data by calendar month, ignoring the year component (e.g. averaging together all Januarys). This allows us to see if some points of the year, on average, are more susceptible to volatile trading patterns.\n",
    "\n",
    "Group the data by month (ignoring the year), and identify :\n",
    "\n",
    "* The month with, on average, the highest volatility\n",
    "* The month with, on average, the lowest volatility\n",
    "* Any general patterns that you notice over the whole year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCkQwdqimW_t",
    "tags": [
     "15_min"
    ]
   },
   "source": [
    "### Exercise 11:\n",
    "\n",
    "The final point that we're interested in is looking at the days where:\n",
    "\n",
    "* The return is high\n",
    "* Trading volume is low\n",
    "\n",
    "This indicates days where the price moved substantially but without much changing hands.\n",
    "\n",
    "The thresholds that we are interested in are:\n",
    "* Low volume: any day with a trading volume in the bottom 25th percentile\n",
    "* High return: any day where the return is in the opt 75th percentile\n",
    "\n",
    "Write the code necessary to:\n",
    "* Calculate and add a \"High/Low\" variable for Volume Level (low is below 25th percentile)\n",
    "* Calculate and add a \"High/Low\" variable for Return (high is above 75th percentile)\n",
    "\n",
    "Describe what you see in terms of:\n",
    "* How many rows fall into our \"low volume\" definition?\n",
    "* How many rows fall into our \"high return\" definition?\n",
    "* How many rows fall into the combination of high return with low volume definition?\n",
    "* What are the 10 rows with the highest return but with low volume? What do you notice about these trades?\n",
    "\n",
    "**Hint:** You can use the [sort_values()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) method in `pandas` to sort a DataFrame by a specific column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPdYuVb9mW_v",
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fRJpHnIEmW_-",
    "tags": [
     "3_min"
    ]
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "tfcr4ugLmW__"
   },
   "source": [
    "Having completed the analysis of the energy sector stock data, we have identified a number of interesting patterns relating volatility to trading volume. Specifically, we found that periods of high volatility also exhibit very high volume. This trend is consistent across all symbols.\n",
    "\n",
    "We also saw that each stock exhibited \"volatility clustering\" – periods of high volatility tend to be clumped together. Each of the stocks experienced high volatility at relatively similar times which suggests some broader market factor may be affecting the energy sector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-KJAuwAmXAC",
    "tags": [
     "5_min"
    ]
   },
   "source": [
    "## Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "QNho4Yn_mXAD"
   },
   "source": [
    "In this case, we've learned the foundations of the ```pandas``` library in Python. We now know how to:\n",
    "\n",
    "1. Read data from CSV files\n",
    "2. Aggregate and manipulate data using ```pandas```\n",
    "3. Analyze summary statistics and gather information from trends across time\n",
    "4. Use ```matplotlib``` to create plots for visual analysis\n",
    "\n",
    "Going forward, we will be consistently using ```pandas``` as a data analysis framework (along with other tools) to build more complex projects and solve critical business problems. It is critical you become as familiar with ```pandas``` as possible and it is imperative that you continue researching/investigating new components of this library after the completion of this program. What we have taught here are only the essential basics of `pandas`; there is still a vast amount of power to the library that you will discover and utilize later on in your development as a data professional.\n",
    "\n",
    "We highly recommend revisiting this case a few more times and going through it from beginning to end with no aid/answers. You should know the various DataFrame/Series methods we introduced here as well as how to carry out common operations on data, such as finding percentiles, before you consider yourself to have \"mastered\" this material."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "case_1.2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
